---
title: "Digit Detection Project"
author: "Isaac Layton"
date: "January 26, 2016"
output: html_document
---

```{r echo=FALSE}

#--------------------------------------------------------------------
kFolds = function(dataset, k) {
  return(sample(1:k, nrow(dataset), replace=TRUE))
}
#--------------------------------------------------------------------
load('~/Desktop/post-grad-learning/digit_detection/.RData')
#--------------------------------------------------------------------
Cols=function(vec) {
  cols=rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
#--------------------------------------------------------------------

```

  The MNIST dataset is a popular introduction into image classification. It was created by the National Institute of Standards and Technology and contains 42,000 gray-scale pixelated digits hand-drawn by 500 participants. Each digit is recorded by measuring each pixels' darkness level; for a 28-by-28 pixel image, this equates to 784 different variables that range from completely white to completely black. The goal of this project was to preprocess the data using dimension reduction, classify the data using machine learning and nonparametric techniques, and estimate the accuracy of the techniques using cross validation.
  
#Data Preprocessing

I began by creating a function to visualize what the digits looked like. Encoding the darkness as hexidecimal values and plotting the points gave a clear look at digits:
  
```{r echo=FALSE}
#--------------------------------------------------------------------------------
pixpos = function(n) {
  y = 27-floor(n/28)
  x = n%%28
  return(c(x,y))
}
#--------------------------------------------------------------------------------
num2hex = function(n) {
  n = 255-n
  first = floor(n/16)
  second = n%%16
  if (first>9) {
    first = rawToChar(as.raw(first+55))
  }
  if (second>9) {
    second = rawToChar(as.raw(second+55))
  }
  both = paste(c(toString(first), toString(second)), collapse="")
  return(paste(c("#",both,both,both),collapse=""))
}
#--------------------------------------------------------------------------------
digitmap = function(col_list) {
  plot(c(0,28), c(0,28), type="n", xlab="Pixel Row", ylab="Pixel Column")
  abline(v=(seq(0,28,1)), col="lightgray")
  abline(h=(seq(0,28,1)), col="lightgray")
  for (i in 2:785) {
    b = pixpos(i-2)
    if (col_list[i]>0) {
      rect(b[1], b[2], b[1]+1, b[2]+2, col=num2hex(col_list[i]),
           border='black')
    }
  }
}
#--------------------------------------------------------------------------------
digitmap(sample_digit)
```

  Because the data contains 42,000 observations and 784 pixel location variables, some row and column reduction was necessary to make computations faster. 
  
  Reducing the number of observations began with splitting the data into a training set and a test set. I decided to put 80% (33,600) of the digits in the training set and 20% (8400) in the test set. From there, I further split the training set into 20% (6,720) as a validation set and 80% (26,880) remaining as a training set. The validation set was choosing the best model and parameters, and the test set was to estimate the model's performance if given more data.

  The first way that I was able to reduce the number of variables was by removing all variables with a variance of 0. There's not much usefulness for a variable that never has different values. This got rid of 76 pixel locations, most of which were the white space surrounding each digit. This leaves us with 708 variables.
  
  From here, I conducted principal components analysis (PCA). Principal component analysis is a method of dimension reduction in which the factors of a dataset are replaced by linear combinations of all of the factors. The first linear combination, or the first principal component, represents the direction in the input space that has the most variance, with the second having the second most variance, and so on. There can as many principal components as there are factors, and they'll each be uncorrelated with each other. Applying principal component analysis to the MNIST dataset allows 90% of the variance to be explained in just 225 variables instead of 708:
```{r echo=FALSE}
plot(cumsum(pve), xlab="Number of principal components", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
abline(h=cumsum(pve)[225])
```

Using PCA also allows you to visualize the data easier. It wouldn't be very informative to plot one pixel against another, and plotting more than 3 pixel locations against each other would be impossible with a single scatterplot. With PCA variables though, since they're linear combinations of every pixel location, we can easily visualize the relationship between the digits. Plotting the first and second principal components against each other, you can start to see some patterns:

```{r echo=FALSE}
graphsubset = kFolds(training,100)
plot(training[graphsubset==1,2:3], col=Cols(training[graphsubset==1,1]),
     pch=19,
       xlab="Principal Component 1",ylab="Principal Component 2", ylim=c(-21,20))
legend("bottom",legend=as.character(0:9), pch=19, col=rainbow(10), horiz=T)
#plot(df[graphsubset==1,c(1,3)], col=Cols(df[graphsubset==1,1]), pch=19,
       #xlab="Principal Component 1",ylab="Principal Component 3", ylim=c(-33,15))
#legend("bottom",legend=as.character(0:9), pch=19, col=rainbow(10), horiz=T)
# plot(pr.out$x[graphsubset==1,c(2,3)], col=Cols(scalable_digs[graphsubset==1,1]), pch=19,
#        xlab="Principal Component 2",ylab="Principal Component 3", ylim=c(-33,15))
# legend("bottom",legend=as.character(1:10), pch=19, col=rainbow(10), horiz=T)
```

#Quadratic Discriminant Analysis


The first method I chose to classify the data with was quadratic discriminant analysis. This method incorporates [Bayes Theorem](https://brilliant.org/wiki/bayes-theorem/) to estimate $\Pr(Y = k{\mid}X = x)$, the probability of a digit being $k$ given the observation's pixel configuration. Bayes Theorem states that
\[
\Pr(Y = k{\mid}X = x) = \frac{\Pr(X = x{\mid}Y = k)\Pr(Y = k)}{\Pr(X = x)}
\]
The first part of the numerator, $\Pr(X = x{\mid}Y = k)$, can be estimated with $k$ different multivariate Normal distributions with the mean and variance being estimated respectively by the sample average and sample covariance matrix for observations that are the $k^{th}$ digit. The second part of the numerator, $\Pr(Y = k)$ is estimated by the proportion of observations that come from digit $k$.

The demominator, $\Pr(X = x)$, can be estimated by summing up all of the $k$ possible numerators. This sum encapsulates the total probability between all digits that the observed value has its specific pixel arrangement.

Using this equation, we use log-likelihood to find which digit is most likely (has the highest probability) given the pixel locations and assign the observation to that digit.

I originally involved the use of the function 'qda' located in the MASS library package, but decided that it was easy enough to incorporate the equation into my own function:

```{r echo=TRUE}
QDAFunc = function(testx, trainx, trainy) {
  y_set = sort(unique(trainy))
  cov_k = by(trainx, trainy, cov)
  cov_inv = lapply(cov_k, solve)
  prop_k = by(trainy, trainy, FUN=function(x) length(x)/length(trainy))
  mean_k = apply(trainx, 2, FUN= function(x) by(unlist(x), trainy, mean))
  subtract = sapply(c(1:length(y_set)), FUN=function(i) (-0.5*(t(mean_k[i,])%*%cov_inv[[i]]%*%mean_k[i,]) 
                                              - (0.5*log(det(cov_k[[i]])))+ log(prop_k[i])))
  QDA_vals = apply(testx, 1, FUN=function(x) y_set[which.max(lapply(c(1:length(y_set)), FUN= function(i) (
    -0.5*(t(x)%*%cov_inv[[i]]%*%as.matrix(x)) + (t(x)%*%cov_inv[[i]]%*%mean_k[i,]) + subtract[i])))])
  return(QDA_vals)
}
```

Using quadratic discriminant analysis on a subset of the digits was a moderately successful way to classify the other subsets. Testing the training set against the validation set, we found the following predictions:

```{r echo=FALSE}
table(qda_model, validation[, 1], dnn=c("QDA Predicted", "Actual"))
kfoldpercentage = sum(qda_model==validation[,1])/length(qda_model)
```

Quadratic discriminant analysis predicted about `r round(kfoldpercentage*100,2)`% of the digits correctly. 

```{r echo=FALSE}
# qda.sub = kFolds(scalable_digs[qda.digitsubset==1,], 50)
# par(mfrow=c(1,2))
# plot(pr.out$x[qda.digitsubset==1,1:2][qda.sub==1,], col=Cols(scalable_digs[qda.digitsubset==1,1][qda.sub==1]),
#      pch=19, xlim=c(-15,18),
#        xlab="Principal Component 1",ylab="Principal Component 2")
# legend("right",legend=as.character(1:10), pch=19, col=rainbow(10))
# plot(pr.out$x[qda.digitsubset==1,1:2][qda.sub==1,], col=Cols(qda.class[[1]][qda.sub==1]),
#      pch=19,xlim=c(-15,18),
#        xlab="Principal Component 1",ylab="Principal Component 2")
```


#Random Forests

```{r echo=FALSE}
load('~/Desktop/post-grad-learning/digit_detection/Forest.RData')
rf.percentage = sum(rf.predict==validation[,1])/length(rf.predict)
```


Another way to classify the data is to use random forests. A decision tree is a method of classifying observations by partioning the input space. It assigns the same prediction to all observations that fall into the same segment of the partition. It's called a tree because it starts with all observations as its "root". It then branches out by choosing the most efficient split in a predictor that will minimize the overall error. It continues doing this until a criterion is met. 

  Random forests build on this concept. In a random forest, multiple decision trees are built using bootstrapped observations. An observation is assigned to the most common class predicted by these trees. Each tree includes only a subset of the predictors to increase reliability. The number of predictors to include in each tree tends to be $\sqrt{p}$ with $p$ being the number of predictors. This ensures that no variable will overpower every tree. 
  
  Using random forests on our data and predicting on the validation set gives an accuracy of `r round(rf.percentage*100,2)`%. Below is the predicted vs. actual digit plot: 
```{r echo=FALSE}
table(rf.predict, validation[, 1], 
      dnn=c("Predicted", "Actual"))
```

#K-Nearest Neighbors

K-Nearest Neighbors was the final way in which the data was classified. K-Nearest neighbors involves takng the $k$ observations that are closest (using Euclidean distance) to the test observation and assigns it the most frequent response value. Here is how the predicted digit compared to the actual digit:
```{r echo=FALSE}
#load('~/Desktop/post-grad-learning/digit_detection/knn.RData')
#knn.digitsubset = digitsubset
#table(knntests[[1]], df[knn.digitsubset==1, 1], dnn=c("KNN Predicted", "Actual"))
#knnpercentages = vector()
#for (i in 1:10){
#  knntests[[i]] = knntests[[i]]-1
#  knnpercentages = c(knnpercentages, sum(knntests[[i]]==df[knn.digitsubset==i,1])/length(knntests[[i]]))
#}
```
Predicting the test digits' values using the 5 closest training digits gave us a cross-validated accuracy of %. 

```{r echo=FALSE}
# knn.sub = kFolds(scalable_digs[knn.digitsubset==1,], 50)
# par(mfrow=c(1,2))
# plot(pr.out[knn.digitsubset==1,1:2][knn.sub==1,], col=Cols(scalable_digs[knn.digitsubset==1,1][knn.sub==1]),
#      pch=19,
#        xlab="Principal Component 1",ylab="Principal Component 2", xlim=c(-25,22))
# legend("right",legend=as.character(1:10), pch=19, col=rainbow(10))
# plot(pr.out[knn.digitsubset==1,1:2][knn.sub==1,], col=Cols(knntests[[1]][knn.sub==1]),
#      pch=19,xlim=c(-25,22),
#        xlab="Principal Component 1",ylab="Principal Component 2")
```

#Conclusion

Given the three different methods chosen to classify the handwritten digits, the one that ended up doing the best (K-nearest neighbors) was the least complex. This is not to say that different complex models could not classify the data better (Yann LeCun of Facebook shows several of these models at http://yann.lecun.com/exdb/mnist/). One particular model that has shown to be successful recently in image recognition is the convolutional neural network (ConvNets). In addition, further pre-processing could have reduced the error. 